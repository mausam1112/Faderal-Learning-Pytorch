{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os \n",
    "import cv2\n",
    "import numpy as np\n",
    "import torch\n",
    "from pathlib import Path\n",
    "from torch.utils.data import Dataset, DataLoader, random_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data_path = './dataset/dataset/train'\n",
    "test_data_path = './dataset/dataset/test'\n",
    "\n",
    "classes_train = os.listdir(train_data_path)\n",
    "classes_test = os.listdir(train_data_path)\n",
    "\n",
    "assert classes_train==classes_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_images_path and test_images_path\n",
    "def get_image_paths(data_path, classes_train):\n",
    "    images_paths = []\n",
    "    for class_ in classes_train:\n",
    "        image_directory = os.path.join(data_path, class_)\n",
    "        images_paths = [os.path.join(image_directory, image_name) for image_name in os.listdir(image_directory)]\n",
    "    return images_paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_image_paths = get_image_paths(train_data_path, classes_train)\n",
    "test_image_paths = get_image_paths(test_data_path, classes_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert all(list(map(lambda filepath: os.path.exists(filepath), train_image_paths)))\n",
    "assert all(list(map(lambda filepath: os.path.exists(filepath), test_image_paths)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(623, 498, 125, 100)"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "thresh = int(0.8*len(train_image_paths))\n",
    "train_img_paths = train_image_paths[:thresh]\n",
    "valid_img_paths = train_image_paths[thresh:] \n",
    "\n",
    "len(train_image_paths), len(train_img_paths), len(valid_img_paths), len(test_image_paths)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Tensor, torch.int32, torch.Size([3, 480, 640]))"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "image = cv2.imread(train_img_paths[0])#.shape\n",
    "image = np.array(image, np.int32)\n",
    "# image\n",
    "\n",
    "image = torch.from_numpy(image)\n",
    "type(image), image.dtype, image.permute(2, 0, 1).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'Closed': 0, 'no_yawn': 1, 'Open': 2, 'yawn': 3}"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class_to_index = {class_name:index for index, class_name in enumerate(classes_train)}\n",
    "class_to_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create Custom Dataset\n",
    "class ClassificationDataset(Dataset):\n",
    "    def __init__(self, image_paths, transform=None) -> None:\n",
    "        self.image_paths = image_paths\n",
    "        self.transform = transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.image_paths)\n",
    "    \n",
    "    def __getitem__(self, index):\n",
    "        image_filepath = self.image_paths[index]\n",
    "        image = cv2.imread(image_filepath)\n",
    "        image = np.array(image, np.float32)\n",
    "\n",
    "        image = torch.from_numpy(image).permute(2, 0, 1)\n",
    "\n",
    "        label = Path(image_filepath).parts[-2]\n",
    "        label = class_to_index[label]\n",
    "\n",
    "        return image, label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = ClassificationDataset(train_img_paths)\n",
    "validation_dataset = ClassificationDataset(valid_img_paths)\n",
    "test_dataset = ClassificationDataset(test_image_paths)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 16\n",
    "num_clients = 6\n",
    "records_per_split = int(len(train_dataset) / num_clients) \n",
    "\n",
    "train_dataset_split = random_split(\n",
    "    train_dataset,\n",
    "    [records_per_split] * num_clients\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loaders = [\n",
    "    DataLoader(split, batch_size=batch_size, shuffle=True)\n",
    "    for split in train_dataset_split\n",
    "]\n",
    "\n",
    "valid_loaders = DataLoader(validation_dataset, batch_size=batch_size, shuffle=True)\n",
    "test_loaders = DataLoader(test_dataset, batch_size=batch_size, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ConvBlock(nn.Module):\n",
    "    def __init__(self, in_c, out_c, k, s, p) -> None: # k, s, p -> Kernel size, s-stride, p-padding\n",
    "        super().__init__()\n",
    "        self.conv = nn.Conv2d(in_c, out_c, k, s, p)\n",
    "        self.pooling = nn.MaxPool2d(2)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # return F.relu(self.conv(x))\n",
    "        return F.relu(self.pooling(self.conv(x)))\n",
    "\n",
    "\n",
    "class ClassificationNetwork(nn.Module):\n",
    "    def __init__(self) -> None:\n",
    "        super().__init__()\n",
    "        self.conv1 = ConvBlock(3, 32, 3, 2, 1)\n",
    "        self.conv2 = ConvBlock(32, 64, 3, 2, 1)\n",
    "        self.conv3 = ConvBlock(64, 128, 3, 2, 1)\n",
    "        self.fc1 = nn.Linear(8960, 128)\n",
    "        self.fc2 = nn.Linear(128, 4)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.conv1(x)\n",
    "        x = self.conv2(x)\n",
    "        x = self.conv3(x)\n",
    "        x = torch.flatten(x, 1)\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = self.fc2(x)\n",
    "        return F.log_softmax(x, dim=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "def client_model_train(client_model, train_loader, optimizer):\n",
    "    client_model.train()\n",
    "    for images, labels in train_loader:\n",
    "        # images, labels = images.cuda(), labels.cuda() # uncomment if using GPU\n",
    "        optimizer.zero_grad()\n",
    "        output = client_model(images)\n",
    "        loss = F.nll_loss(output, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "    # print(f\"{loss.item()}\")\n",
    "    return loss.item()\n",
    "\n",
    "\n",
    "def server_model_gradient_aggregation(global_model, client_models):\n",
    "    global_model_state = global_model.state_dict()\n",
    "    for k in global_model_state.keys():\n",
    "        global_model_state[k] = torch.stack(\n",
    "            [client_models[i].state_dict()[k] for i in range(len(client_models))], 0\n",
    "        ).mean(0)\n",
    "    global_model.load_state_dict(global_model_state)\n",
    "    client_models_updated = [model.load_state_dict(global_model.state_dict()) for model in client_models]\n",
    "    return global_model, client_models_updated\n",
    "        \n",
    "\n",
    "\n",
    "def test(global_model, test_loader):\n",
    "    global_model.eval()\n",
    "    test_loss, correct_pred = 0, 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for images, labels in test_loader:\n",
    "            # images, labels = images.cuda(), labels.cuda() # uncomment if using GPU\n",
    "            output = global_model(images)\n",
    "            test_loss += F.nll_loss(output, labels, reduction='sum').item() # add up loss from each batch\n",
    "            pred = output.argmax(dim=1, keepdim=True)\n",
    "            correct_pred += pred.eq(labels.view_as(pred)).sum().item()\n",
    "    \n",
    "    test_loss /= len(test_loader)\n",
    "    acc = correct_pred / len(test_loader)\n",
    "\n",
    "    return test_loss, acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "global_model = ClassificationNetwork()\n",
    "optimizers = [optim.SGD(global_model.parameters(), lr=0.0000001) for _ in range(num_clients)]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llm",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
